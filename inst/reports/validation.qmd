---
title: "Model Validation"
params:
    path: ""
execute:
  warning: false
  message: false
format:
  html:
    theme: flatly
  pdf:
    fontsize: 11pt
df-print: kable
fig-width: 6
fig-height: 6
---

```{r}
#| echo: false

# Helper function to display a plot in Plotly if the report is in HTML format.
render_plot <- function(p) {
    if (knitr::is_html_output()) {
        plotly::ggplotly(p)
    } else {
        p
    }
}
```

# Data Loading

```{r}
results <- arrow::read_csv_arrow(file.path(params$path, "results.csv"))
rmse <- dplyr::filter(results, metric == "rmse")
aicc <- dplyr::filter(results, metric == "aicc")
```

{{< pagebreak >}}

# Models
```{r}
models <- list(
    # ARIMA default
    arima_def = fable::ARIMA(sqrt(units)),
    # ARIMA default with predictors
    arima_base = fable::ARIMA(sqrt(units) ~ feature + display + tpr_only),
    # ARIMA with lagged predictors
    arima_lagged = fable::ARIMA(
        sqrt(units) ~ feature + display + tpr_only +
            dplyr::lag(feature) + dplyr::lag(display) + dplyr::lag(tpr_only)
    ),
    # ARIMA with seasonal predictors
    arima_seasonal = fable::ARIMA(
        sqrt(units) ~ PDQ(0, 0, 0) + fourier(K = 6) +
            feature + display + tpr_only
    ),
    # ARIMA with seasonal and lagged predictors
    arima_seasonal_lagged = fable::ARIMA(
        sqrt(units) ~ PDQ(0, 0, 0) + pdq(d = 0) +
            fourier(K = 6) + feature + display + tpr_only +
            dplyr::lag(feature) + dplyr::lag(display) + dplyr::lag(tpr_only)
    ),
    # Seasonal decomposition model with ETS errors.
    stl = fabletools::decomposition_model(
        feasts::STL(sqrt(units)),
        fable::ETS(season_adjust ~ season("N"))
    ),
    # Default Neural Network Model with predictors.
    nnetar = fable::NNETAR(sqrt(units) ~ feature + display + tpr_only),
    # Default prophet model with predictors.
    prophet = fable.prophet::prophet(
        sqrt(units) ~ feature + display + tpr_only
    )
)
```

# Evaluation
## AICc 

```{r}
#| echo: !expr knitr::is_html_output()
#| code-fold: true

# Create a column called `best_model` with the arima model which minimizes AICc, then count them.
# Also, add the mean AICc among all models.
arima_model_names <- names(models[grepl("arima", names(models))])
plt <- aicc |>
    dplyr::ungroup() |>
    dplyr::rowwise() |>
    dplyr::mutate(
        mean_aicc = mean(
            dplyr::c_across(tidyselect::all_of(arima_model_names)),
            na.rm = TRUE
        ),
        best_model = arima_model_names[
            which.min(dplyr::c_across(tidyselect::all_of(arima_model_names)))
        ]
    ) |>
    dplyr::group_by(best_model) |>
    dplyr::summarise(
        mean_aicc = mean(mean_aicc),
        n_best_model = dplyr::n()
    ) |>
    dplyr::mutate(mean_aicc_text = paste("Mean AICc: ", round(mean_aicc, 2))) |>
    ggplot2::ggplot(ggplot2::aes(best_model, n_best_model, fill = mean_aicc)) +
    ggplot2::geom_bar(stat = "identity", show.legend = FALSE) +
    ggplot2::geom_text(ggplot2::aes(label = mean_aicc_text), vjust = -0.5) +
    ggplot2::labs(
        title = "Best ARIMA models by AICc",
        y = "Number of best models by AICc",
        caption = glue::glue("Models: {paste(arima_model_names, collapse = ', ')}"),
    )

render_plot(plt)
```

- The best model among arima variations is the lagged version of ARIMA.
- Prophet model has more than 60% of null models so I will remove it from the mable.
- Because NNETAR is very slow at predicting, I will remove it from the mable. Also, is not performing well in the validation set due to lack of historic data.

{{< pagebreak >}}

## RMSE

Plot best models count by minimum RMSE among models with their median.

```{r}
#| echo: !expr knitr::is_html_output()
#| code-fold: true
mods <- c("arima_lagged", "stl")
plt <- rmse |>
    dplyr::ungroup() |>
    dplyr::rowwise() |>
    dplyr::mutate(
        best_model = mods[
            which.min(dplyr::c_across(mods))
        ],
        mean_rmse = mean(
            dplyr::c_across(c("arima_lagged", "stl")),
            na.rm = TRUE
        )
    ) |>
    dplyr::group_by(best_model) |>
    dplyr::summarise(
        # The median is used instead of the mean
        # because it's more robust to outliers
        median_rmse = median(mean_rmse),
        n_best_model = dplyr::n()
    ) |>
    dplyr::mutate(
        median_rmse_text = paste("Median RMSE: ", round(median_rmse, 2))
    ) |>
    ggplot2::ggplot(
        ggplot2::aes(
            reorder(best_model, n_best_model, decreasing = TRUE),
            n_best_model,
            fill = median_rmse
        )
    ) +
    ggplot2::geom_bar(stat = "identity", show.legend = FALSE) +
    ggplot2::geom_text(ggplot2::aes(label = median_rmse_text), vjust = -0.5) +
    ggplot2::labs(
        x = NULL,
        y = "Number of best models by RMSE",
        title = "Models performance in validation set"
    )

render_plot(plt)
```

Based on RMSE, ARIMA lagged model with predictors performs better than STL decomposition model with ETS errors in the majority of the validation set.
Anyway, both perform well in the validation set, so I will consider apply both models.